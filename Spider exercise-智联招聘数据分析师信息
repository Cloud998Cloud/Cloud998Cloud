# 导入各种需要的包
import requests
from lxml import etree
import numpy as np
import pandas as pd
import time

# 获取爬取的城市和爬取的页码数
def get_city_page():
    city = input('请输入要爬取的城市：')
    page = int(input('请输入要爬取的页码数：'))
    return city, page

# 构建url_list
def create_url_lis(city, page):
    # 城市对应编码
    city_code_dict = {
        '上海':538, '北京':530, '广州':763, '深圳':765, '天津':531, '武汉':736, '西安':854, 
        '成都':801, '南京':635, '杭州':653, '重庆':551, '厦门':682
    }
    # 定义一个空列表用于存放url
    url_lis = []
    # 根据城市中文名取出对应的城市编码
    city_code = city_code_dict[city]
    # 循环遍历页码数，生产url_list
    for p in range(page):
        url = 'https://sou.zhaopin.com/?jl={}&kw={}&p={}'.format(city_code, '文员', p+1)
        url_lis.append(url)
    return url_lis
        
        
# 以下代码为测试用
# city, page = get_city_page()
# url_lis = create_url_lis(city, page)
# url_lis

# 根据url获取网页源代码
def get_html(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',
        'cookie': 'x-zp-client-id=46b2defe-afc7-4d40-ea46-6590215e9154; urlfrom2=121114589; adfcid2=cn.bing.com; adfbid2=0; FSSBBIl1UgzbN7NO=55yArBe96FLiC98sMBqzEqEb3etFwbRcdfwdCt_oK4uMaeQfRuzJHn83bdlkBDHoEfdrfq1ie4nIj4yHeCqEO5q; _uab_collina=164947365030347125929782; sts_deviceid=1800c4f0fa7b7d-083eb366d25eb9-1f343371-921600-1800c4f0fa8a37; dywea=95841923.851422850947368700.1649507974.1649507974.1649507974.1; dywez=95841923.1649507974.1.1.dywecsr=(direct)|dyweccn=(direct)|dywecmd=(none)|dywectr=undefined; __utma=269921210.358179048.1649507974.1649507974.1649507974.1; __utmz=269921210.1649507974.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); urlfrom=121114589; adfcid=cn.bing.com; adfbid=0; Hm_lvt_38ba284938d5eddca645bb5e02a02006=1655623432; locationInfo_search={%22code%22:%22538%22%2C%22name%22:%22%E4%B8%8A%E6%B5%B7%22%2C%22message%22:%22%E5%8C%B9%E9%85%8D%E5%88%B0%E5%B8%82%E7%BA%A7%E7%BC%96%E7%A0%81%22}; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%221083031295%22%2C%22first_id%22%3A%221800c4a4c5c8ea-09463bb2be54fb8-1f343371-921600-1800c4a4c5d172%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%2C%22%24latest_referrer%22%3A%22%22%7D%2C%22%24device_id%22%3A%221800c4a4c5c8ea-09463bb2be54fb8-1f343371-921600-1800c4a4c5d172%22%7D; ssxmod_itna=eqmODvxUxfhN0QDXDnQmqG=UQxzj970UKDBk7eAQDyD8xA3GEmCEtDGr+DvGxVG4TVIWYvoPlirB7KCDR2etE7mDB3DE244h3bDeeDvDCeDIDWeDiDG4Gm7xGtaD7qDdEsds/8DbxYpnGxDmDGAnKqDgDYQDGqDn04G2D7UnYedeaobBGG0xKY=DjkbD/+xb2o=KdZWCUw0tVnNTKqGyfPGuITPl7obDCh6lAuX1A0e4Ai41ni4Kix4ez+4CGYqpo7wLaD3dmD4PADlx0oEqf+gpxD3RHKqxD===; ssxmod_itna2=eqmODvxUxfhN0QDXDnQmqG=UQxzj970KD8LmDDIQD/GDRgD7P6Oc9gWgjxF+QOD8h4=P/nPla2w4diLpfI3F4OGiNdEiiYeYNOrhxH+YC==Yxp83x7jWjDjKD2boD===; ZL_REPORT_GLOBAL={%22jobs%22:{%22funczoneShare%22:%22dtl_best_for_you%22%2C%22recommandActionidShare%22:%226080613d-888d-41c0-b0c2-9c81a86b91ce-job%22}}; zp_passport_deepknow_sessionId=7428cbc1scae724b21a198b313cc3a09974c; at=71d0f702115c4f9fba42e0c0507d1a3b; rt=e78e78affd9248e9a30175ee0380570c; Hm_lpvt_38ba284938d5eddca645bb5e02a02006=1655628250; FSSBBIl1UgzbN7NP=53j4fCDtXgwGqqqDr86a41qYR103TORuFFk35VMvKQtOVlpBULdigR7cbLcjQ3HBVxQnhMon9ROvj_xqX9oOQ2lBu9oeibNJj1C7nI5FUMliVGar5g8M.Wy0XL.RraDoNmGp.2kxUC1AScrDuVdl9H6BUHr710qY2wKDem6tvPzBsgmx86IfKSODMjSkabkdjlw1pt3GyqWtUVzUbhf6DaDT4nKzu1aFE2yKQ_ineiAptd_dfDkgF7g6k.9taZRCFKCyJrDpDuxeMlGTqNx123GNfxl0xit_rl.xm9ypGpUCtvLD1tpcnkp77yy3k34oGpxxhmeWvjhpKyGWHWffgtv'
    }
    html = requests.get(url,headers = headers).text
    return html

# 以下为测试代码
# url = 'https://sou.zhaopin.com/?jl=531&kw=%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90&p=1'
# res = get_html(url)
# print(res[0:600])

# 解析网页源代码，提取想要的信息
def transform_html(response):
    """
    解析网页源代码，提取想要的信息，并返回信息的dataframe
    response：抓取到的网页源代码
    """
    # 职位名称、薪资、地区、工作经验、学历、职位类别、招聘人数、职位描述、岗位职责、职位福利、公司名称、公司类型、公司规模
    html = etree.HTML(response)
    
    # 获取工作名称
    job = html.xpath('//span[@class="iteminfo__line1__jobname__name"]/@title')
    
    # 获取薪资范围
    salary = html.xpath('//p[@class="iteminfo__line2__jobdesc__salary"]/text()')
    for i in range(len(salary)):
        salary[i] = salary[i].strip('\n').strip(' ').rstrip('\n')
        
    # 获取地区、经验、学历信息
    location, experience, education = ([] for i in range(3))
    require = html.xpath('//ul[@class="iteminfo__line2__jobdesc__demand"]')
    for req in require:
        try:
            loc = req.xpath('.//li[@class="iteminfo__line2__jobdesc__demand__item"]/text()')[0]
            location.append(loc)
        except:
            location.append(np.nan)
        try:
            exp = req.xpath('.//li[@class="iteminfo__line2__jobdesc__demand__item"]/text()')[1]
            experience.append(exp)
        except:
            experience.append(np.nan)
        try:
            edu = req.xpath('.//li[@class="iteminfo__line2__jobdesc__demand__item"]/text()')[2]
            education.append(edu)
        except:
            education.append(np.nan)
            
    # 获取职位标签
    job_tag = []
    job_tag_lis = html.xpath('//div[@class="iteminfo__line3__welfare"]')
    for tag in job_tag_lis:
        tag_info = tag.xpath('.//div[@class="iteminfo__line3__welfare__item"]/text()')
        tag_info = str(tag_info)
        job_tag.append(tag_info)
        
    #获取公司名称
    company_name = html.xpath('//span[@class="iteminfo__line1__compname__name"]/text()')
    
    #获取公司类型、公司规模
    company_type, company_size = [], []
    company_detail = html.xpath('//div[@class="iteminfo__line2__compdesc"]')
    for company in company_detail:
        try:
            com_type = company.xpath('.//span[@class="iteminfo__line2__compdesc__item"]/text()')[0]
            company_type.append(com_type)
        except:
            company_type.append(np.nan)
        try:
            com_size = company.xpath('.//span[@class="iteminfo__line2__compdesc__item"]/text()')[1]
            company_size.append(com_size)
        except:
            company_size.append(np.nan)
    
    data_lis = [job, salary, location, experience, education, job_tag, company_name, company_type, company_size]
    
    # 爬取结果合成一个dataframe
    get_data = pd.DataFrame(columns=['职位名称','薪资范围','地点','工作经验','学历要求','岗位标签','公司名称','公司类型','公司规模'])
    for col, data in zip(get_data, data_lis):
        get_data[col] = data
    # 返回数据的DataFrame
    return get_data

# 以下为测试用代码
# get_data = transform_html(res)
# get_data.head(5)

# 循环爬取每一页的数据，并且合成为一个dataframe
def concat_data(url_lis):
    # 定义字典储存dataframe
    final_df_dict = {}
    for url,num in zip(url_lis,range(len(url_lis))):
        try:
            print('开始爬取第{}页'.format(num+1))
            # 获取网页源代码
            response = get_html(url)
            # 解析网页源代码并且生成一个dataframe
            final_df = transform_html(response)
            # 将dataframe保存到字典里
            final_df_dict[num] = final_df
            print('第{}页爬取完成'.format(num+1))
            # 爬取完成后程序休眠8秒
            time.sleep(8)
        except:
            print('所有页码都爬取完成！总计爬取{}页'.format(num+1))
    concat_df = pd.concat(list(final_df_dict.values()),ignore_index = True)
    return concat_df

# 爬取结果保存到csv
def save_df(df,city):
    file_name = '{}文员招聘信息.csv'.format(city)
    path = r'D:\Python_exec\爬虫\智联招聘信息爬取\{}'.format(file_name)
    df.to_csv(path,encoding = 'utf-8-sig',index = False)
    print('{}保存成功'.format(file_name))
    return 



# 调用主函数，执行上边所有函数
def main():
    city,page = get_city_page()
    url_lis = create_url_lis(city,page)
    concat_df = concat_data(url_lis)
    save_df(concat_df,city)
    print('所有程序执行完毕')

main()

